{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Sentiment Analyser For Movie Reviews Using NLTK-Python-TFlearn (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5331/5331 [00:04<00:00, 1200.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5331/5331 [00:05<00:00, 922.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create My own Lexicon for data preprocessing \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "word_list = []\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "with open('D:\\\\sentiment_analysis\\\\pos.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "for i in tqdm(range(0,len(content))):\n",
    "    tokens = (word_tokenize(content[i]))\n",
    "    for word in tokens:\n",
    "        word = lem.lemmatize(word)\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "\n",
    "                  \n",
    "with open('D:\\\\sentiment_analysis\\\\neg.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "for i in tqdm(range(0,len(content))):\n",
    "    tokens = word_tokenize(content[i])   \n",
    "    for word in tokens:\n",
    "        word = lem.lemmatize(word)\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "\n",
    "        \n",
    "array = np.array(word_list)\n",
    "np.save('dict.npy',array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/5331 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 1/5331 [00:00<56:51,  1.56it/s]\n",
      "  0%|                                                                                 | 2/5331 [00:01<58:30,  1.52it/s]\n",
      "  0%|                                                                                 | 4/5331 [00:01<46:36,  1.90it/s]\n",
      "  0%|                                                                                 | 5/5331 [00:02<43:07,  2.06it/s]\n",
      "  0%|                                                                                 | 6/5331 [00:02<41:34,  2.13it/s]\n",
      "  0%|                                                                                 | 7/5331 [00:02<33:25,  2.65it/s]\n",
      "  0%|                                                                                 | 8/5331 [00:03<32:37,  2.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5331/5331 [33:05<00:00,  3.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfunction()\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Preprocess the data set as per the lexicon above.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "lexicon = np.load('dict.npy')\n",
    "\n",
    "# for positive sentiment data-set.\n",
    "\n",
    "with open('D:\\\\sentiment_analysis\\\\pos.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "pos = np.zeros((len(content),int(lexicon.size)), dtype=np.int)    \n",
    "\n",
    "for i in tqdm(range(0,len(content))):\n",
    "    sent = content[i]\n",
    "    sent = word_tokenize(sent)\n",
    "    for word in sent:\n",
    "        word = lem.lemmatize(word)\n",
    "        for j in range(0,int(lexicon.size)):\n",
    "            if str(word) == str(lexicon[j]):\n",
    "                pos[i][j] +=1\n",
    "\n",
    "\n",
    "np.save('pos-senti-preprocessed.npy',pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5331/5331 [32:22<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for negative sentiment data-set.\n",
    "\n",
    "lexicon = np.load('dict.npy')\n",
    "\n",
    "\n",
    "with open('D:\\\\sentiment_analysis\\\\neg.txt') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "neg = np.zeros((len(content),int(lexicon.size)), dtype=np.int)    \n",
    "\n",
    "for i in tqdm(range(0,len(content))):\n",
    "    sent = content[i]\n",
    "    sent = word_tokenize(sent)\n",
    "    for word in sent:\n",
    "        word = lem.lemmatize(word)\n",
    "        for j in range(0,int(lexicon.size)):\n",
    "            if str(word) == str(lexicon[j]):\n",
    "                neg[i][j] +=1\n",
    "\n",
    "\n",
    "np.save('neg-senti-preprocessed.npy',neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5331/5331 [00:00<00:00, 36417.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5331/5331 [00:00<00:00, 15257.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2, 1, 1, ..., 0, 0, 0]) array([1, 0])\n",
      " array([3, 0, 1, ..., 0, 0, 0]) ..., array([0, 1])\n",
      " array([0, 0, 1, ..., 0, 0, 0]) array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# combine the data after preprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "final = []\n",
    "\n",
    "pos = np.load('pos-senti-preprocessed.npy')\n",
    "pos_one_hot = np.array([1,0])\n",
    "\n",
    "for i in tqdm(range(0,5331)):\n",
    "    final = np.append(final,(pos[i],pos_one_hot))\n",
    "    \n",
    "neg = np.load('neg-senti-preprocessed.npy')\n",
    "neg_one_hot = np.array([0,1])\n",
    "\n",
    "for i in tqdm(range(0,5331)):\n",
    "    final = np.append(final,(neg[i],neg_one_hot))\n",
    "\n",
    "print(final)\n",
    "final = final.reshape(10662,2)\n",
    "np.random.shuffle(final)\n",
    "np.save('Final-preprocessed-sentiment-data',final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "Scipy not supported!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1669  | total loss: 0.08413 | time: 7.491s\n",
      "| Adam | epoch: 010 | loss: 0.08413 - acc: 0.9940 -- iter: 10624/10662\n",
      "Training Step: 1670  | total loss: 0.07613 | time: 7.534s\n",
      "| Adam | epoch: 010 | loss: 0.07613 - acc: 0.9946 -- iter: 10662/10662\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Shekhar Shiroor\\NLP-sentiment-analysis-CNN is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "IMG_SIZE = 1\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'NLP-sentiment-analysis-CNN'\n",
    "\n",
    "array = np.load('Final-preprocessed-sentiment-data-shuffled.npy')\n",
    "array = array.reshape(10662,2)\n",
    "\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE,18669], name='input')\n",
    "'''\n",
    "convnet =  tflearn.lstm(convnet, 1024, activation='relu', return_seq=True)\n",
    "\n",
    "convnet =  tflearn.lstm(convnet, 512, activation='relu')\n",
    "'''\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 128, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 512, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 256, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 128, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "\n",
    "X = np.array([i[0] for i in array]).reshape(-1,IMG_SIZE,IMG_SIZE,18669)\n",
    "Y = [i[1] for i in array]\n",
    "\n",
    "model.fit({'input': X}, {'targets': Y}, n_epoch=10, snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n",
    "\n",
    "model.save(MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for the custom data from the User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "Scipy not supported!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "I am happy with the movie\n",
      "\n",
      " The Sentiment is Positive \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "y\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "I am not that happy with the movie\n",
      "\n",
      " The Sentiment is Positive \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "y\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "The movie was good but could be better\n",
      "\n",
      " The Sentiment is Negative \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "IMG_SIZE = 3\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'NLP-sentiment-analysis'\n",
    "\n",
    "convnet = input_data(shape=[None, IMG_SIZE, 6223], name='input')\n",
    "\n",
    "convnet =  tflearn.lstm(convnet, 1024, activation='relu', return_seq=True)\n",
    "\n",
    "convnet =  tflearn.lstm(convnet, 512, activation='relu')\n",
    "\n",
    "convnet = fully_connected(convnet, 512, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 256, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 128, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "\n",
    "lexicon = np.load('dict.npy')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print('\\nEnter the sentence for testing its sentiment by Neural Neywork Model !...\\n')\n",
    "    string = str(input())\n",
    "    arr = np.zeros((1,int(lexicon.size)), dtype=np.int)\n",
    "    sent = word_tokenize(string)\n",
    "    for word in sent:\n",
    "        word = lem.lemmatize(word)\n",
    "        for j in range(0,int(lexicon.size)):\n",
    "            if str(word) == str(lexicon[j]):\n",
    "                arr[0][j] +=1\n",
    "    \n",
    "    arr = arr.reshape(-1,3,6223)\n",
    "    model_out = model.predict(arr)\n",
    "    \n",
    "    if np.argmax(model_out) == 0:\n",
    "        print('\\n The Sentiment is Positive ')\n",
    "    else:\n",
    "        print('\\n The Sentiment is Negative ')\n",
    "        \n",
    "    print('\\n Would like to test on another sentence ? \\n')\n",
    "    t = str(input())\n",
    "    if t == 'N' or t == 'No' or t == 'no' or t == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "I am not that happy with the movie\n",
      "\n",
      " The Sentiment is Positive \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "yes\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "The movie could have been better\n",
      "\n",
      " The Sentiment is Negative \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "yes\n",
      "\n",
      "Enter the sentence for testing its sentiment by Neural Neywork Model !...\n",
      "\n",
      "The movie was up to the mark\n",
      "\n",
      " The Sentiment is Negative \n",
      "\n",
      " Would like to test on another sentence ? \n",
      "\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# NLP By Using CNN Model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "IMG_SIZE = 1\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'NLP-sentiment-analysis-CNN'\n",
    "\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE,18669], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 128, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 512, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 256, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 128, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "\n",
    "lexicon = np.load('dict.npy')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print('\\nEnter the sentence for testing its sentiment by Neural Neywork Model !...\\n')\n",
    "    string = str(input())\n",
    "    arr = np.zeros((1,int(lexicon.size)), dtype=np.int)\n",
    "    sent = word_tokenize(string)\n",
    "    for word in sent:\n",
    "        word = lem.lemmatize(word)\n",
    "        for j in range(0,int(lexicon.size)):\n",
    "            if str(word) == str(lexicon[j]):\n",
    "                arr[0][j] +=1\n",
    "    \n",
    "    arr = arr.reshape(-1,IMG_SIZE,IMG_SIZE,18669)\n",
    "    model_out = model.predict(arr)\n",
    "    \n",
    "    if np.argmax(model_out) == 0:\n",
    "        print('\\n The Sentiment is Positive ')\n",
    "    else:\n",
    "        print('\\n The Sentiment is Negative ')\n",
    "        \n",
    "    print('\\n Would like to test on another sentence ? \\n')\n",
    "    t = str(input())\n",
    "    if t == 'N' or t == 'No' or t == 'no' or t == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of data in Tensorboard (PCA and t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 170517.49it/s]\n"
     ]
    }
   ],
   "source": [
    "array = np.load('Final-preprocessed-sentiment-data-shuffled.npy')\n",
    "\n",
    "file = open( 'metadata-NLP.tsv', 'w')\n",
    "file.write('Name\\tClass\\n')\n",
    "\n",
    "for i in tqdm(range(0,2000)):\n",
    "    temp = array[i][1]\n",
    "    if(temp[0]==1):\n",
    "        file.write('%s\\n' % ('Positive'))\n",
    "    else:\n",
    "        file.write('%s\\n' % ('Negative'))\n",
    "        \n",
    "file.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:36<00:00,  5.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./log-NLP/model.ckpt-0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "array = np.load('Final-preprocessed-sentiment-data-shuffled.npy')\n",
    "temp = []\n",
    "\n",
    "for i in tqdm(range(0,2000)):\n",
    "    temp = np.append(temp,array[i][0])\n",
    "\n",
    "temp = temp.reshape(2000,18669)    \n",
    "\n",
    "embedding_variable = tf.Variable(temp, name='embedding')\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./log-NLP')\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_variable.name\n",
    "embedding.metadata_path = 'metadata-NLP.tsv'\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, './log-NLP/model.ckpt', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
